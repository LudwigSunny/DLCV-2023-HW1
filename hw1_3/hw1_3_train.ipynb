{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.004804,"end_time":"2023-10-06T03:40:01.908042","exception":false,"start_time":"2023-10-06T03:40:01.903238","status":"completed"},"tags":[]},"source":["# Set up packages for HW1\n","# Trained on Kaggle"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-14T17:28:12.676762Z","iopub.status.busy":"2023-10-14T17:28:12.676310Z","iopub.status.idle":"2023-10-14T17:28:21.989292Z","shell.execute_reply":"2023-10-14T17:28:21.988227Z","shell.execute_reply.started":"2023-10-14T17:28:12.676723Z"},"papermill":{"duration":129.979914,"end_time":"2023-10-06T03:42:11.891842","exception":false,"start_time":"2023-10-06T03:40:01.911928","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gdown\n","  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\n","Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","Installing collected packages: gdown\n","Successfully installed gdown-4.7.1\n"]}],"source":["!pip install --upgrade gdown tqdm"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.025511,"end_time":"2023-10-06T03:42:11.943167","exception":false,"start_time":"2023-10-06T03:42:11.917656","status":"completed"},"tags":[]},"source":["# About the Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:21.991867Z","iopub.status.busy":"2023-10-14T17:28:21.991547Z","iopub.status.idle":"2023-10-14T17:28:45.021213Z","shell.execute_reply":"2023-10-14T17:28:45.019873Z","shell.execute_reply.started":"2023-10-14T17:28:21.991835Z"},"papermill":{"duration":36.616435,"end_time":"2023-10-06T03:42:48.585319","exception":false,"start_time":"2023-10-06T03:42:11.968884","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From (uriginal): https://drive.google.com/uc?id=1Ijt2MqvY9knLkLYcgjaUwSUOClWhtX37\n","From (redirected): https://drive.google.com/uc?id=1Ijt2MqvY9knLkLYcgjaUwSUOClWhtX37&confirm=t&uuid=a9fabc91-66c6-426a-9610-c7f565de9fa7\n","To: /kaggle/working/hw1_data.zip\n","100%|███████████████████████████████████████| 1.13G/1.13G [00:04<00:00, 259MB/s]\n"]}],"source":["# Download dataset\n","!gdown 1Ijt2MqvY9knLkLYcgjaUwSUOClWhtX37 -O hw1_data.zip\n","\n","# Unzip the downloaded zip file\n","# This may take some time.\n","!unzip -q ./hw1_data.zip"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:45.023646Z","iopub.status.busy":"2023-10-14T17:28:45.023033Z","iopub.status.idle":"2023-10-14T17:28:48.770158Z","shell.execute_reply":"2023-10-14T17:28:48.769282Z","shell.execute_reply.started":"2023-10-14T17:28:45.023610Z"},"papermill":{"duration":2.14947,"end_time":"2023-10-06T03:42:50.765956","exception":false,"start_time":"2023-10-06T03:42:48.616486","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Import necessary packages.\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import os\n","import imageio\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from PIL import Image #PIL包含在pillow這個函式庫\n","# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n","from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n","from torchvision.datasets import DatasetFolder, VisionDataset\n","# This is for the progress bar.\n","from tqdm import tqdm\n","import random"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:48.773350Z","iopub.status.busy":"2023-10-14T17:28:48.772617Z","iopub.status.idle":"2023-10-14T17:28:51.567048Z","shell.execute_reply":"2023-10-14T17:28:51.565908Z","shell.execute_reply.started":"2023-10-14T17:28:48.773315Z"},"trusted":true},"outputs":[],"source":["num_classes = 7\n","def bilinear_kernel(in_channels, out_channels, kernel_size):\n","    factor = (kernel_size + 1) // 2\n","    if kernel_size % 2 == 1:\n","        center = factor - 1\n","    else:\n","        center = factor - 0.5\n","    og = (torch.arange(kernel_size).reshape(-1, 1),\n","          torch.arange(kernel_size).reshape(1, -1))\n","    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n","           (1 - torch.abs(og[1] - center) / factor)\n","    weight = torch.zeros((in_channels, out_channels,\n","                          kernel_size, kernel_size))\n","    weight[range(in_channels), range(out_channels), :, :] = filt\n","    return weight\n","\n","vgg16_backbone = models.vgg16(weights=None)\n","W = bilinear_kernel(num_classes, num_classes, 64)\n","\n","class fcn32s(nn.Module):\n","    def __init__(self, n_class=21):\n","        super(fcn32s, self).__init__()\n","        # vgg16\n","        self.features = vgg16_backbone.features # 1/32\n","\n","        # fc6\n","        self.fc6 = nn.Conv2d(512, 4096, 7, padding=3)\n","        self.relu6 = nn.ReLU(inplace=True)\n","        self.drop6 = nn.Dropout2d()\n","\n","        # fc7\n","        self.fc7 = nn.Conv2d(4096, 4096, 1)\n","        self.relu7 = nn.ReLU(inplace=True)\n","        self.drop7 = nn.Dropout2d()\n","        \n","        # Conv2d\n","        self.score_fr = nn.Conv2d(4096, n_class, 1)\n","        \n","        # ConvTranspose2d\n","        self.upscore = nn.ConvTranspose2d(n_class, n_class, kernel_size=64, stride=32, padding=16)\n","        \n","        # 全卷积网络用双线性插值的上采样初始化转置卷积层\n","        self.upscore.weight.data.copy_(W)\n","    \n","\n","    def forward(self, x):\n","        \n","        h = self.features(x)\n","        \n","        h = self.relu6(self.fc6(h))\n","        h = self.drop6(h)\n","\n","        h = self.relu7(self.fc7(h))\n","        h = self.drop7(h)\n","\n","        h = self.score_fr(h)\n","\n","        h = self.upscore(h)\n","        # h = h[:, :, 19:19 + x.size()[2], 19:19 + x.size()[3]].contiguous()\n","\n","        return h"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:51.575569Z","iopub.status.busy":"2023-10-14T17:28:51.572748Z","iopub.status.idle":"2023-10-14T17:28:52.036754Z","shell.execute_reply":"2023-10-14T17:28:52.035902Z","shell.execute_reply.started":"2023-10-14T17:28:51.575528Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n","100%|██████████| 21.1M/21.1M [00:00<00:00, 201MB/s]\n"]}],"source":["\"\"\" DeepLabv3 Model download and change the head for your prediction\"\"\"\n","from torchvision.models.segmentation.deeplabv3 import DeepLabHead, FCNHead\n","mv3_backbone = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1, dilated=True)\n","mv3_backbone = mv3_backbone.features\n","stage_indices = [0] + [i for i, b in enumerate(mv3_backbone) if getattr(b, \"_is_cn\", False)] + [len(mv3_backbone) - 1]\n","out_pos = stage_indices[-1]  # use C5 which has output_stride = 16\n","out_inplanes = mv3_backbone[out_pos].out_channels\n","\n","aux_pos = stage_indices[-4]  # use C2 here which has output_stride = 8\n","aux_inplanes = mv3_backbone[aux_pos].out_channels\n","\n","def createDeepLabv3_mv3(outputchannels):\n","    \"\"\"DeepLabv3 class with custom head\n","    Args:\n","        outputchannels (int, optional): The number of output channels\n","        in your dataset masks. Defaults to 1.\n","    Returns:\n","        model: Returns the DeepLabv3 model with the ResNet101 backbone.\n","    \"\"\"\n","    model = models.segmentation.deeplabv3_mobilenet_v3_large(weights=models.segmentation.DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT,\n","                                                    progress=True)\n","\n","    model.aux_classifier = FCNHead(aux_inplanes, outputchannels)\n","    model.classifier = DeepLabHead(out_inplanes, outputchannels)\n","\n","    return model\n","\n","\n","def createDeepLabv3_r50(outputchannels):\n","    \"\"\"DeepLabv3 class with custom head\n","    Args:\n","        outputchannels (int, optional): The number of output channels\n","        in your dataset masks. Defaults to 1.\n","    Returns:\n","        model: Returns the DeepLabv3 model with the ResNet101 backbone.\n","    \"\"\"\n","    model = models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT,\n","                                                    progress=True)\n","\n","    model.classifier = DeepLabHead(2048, outputchannels)\n","\n","    return model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:52.038934Z","iopub.status.busy":"2023-10-14T17:28:52.038303Z","iopub.status.idle":"2023-10-14T17:28:52.070492Z","shell.execute_reply":"2023-10-14T17:28:52.069942Z","shell.execute_reply.started":"2023-10-14T17:28:52.038901Z"},"papermill":{"duration":0.12921,"end_time":"2023-10-06T03:42:50.926168","exception":false,"start_time":"2023-10-06T03:42:50.796958","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# set a random seed for reproducibility\n","myseed = 6666  \n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(myseed)\n","torch.manual_seed(myseed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(myseed)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:52.072557Z","iopub.status.busy":"2023-10-14T17:28:52.071866Z","iopub.status.idle":"2023-10-14T17:28:52.079285Z","shell.execute_reply":"2023-10-14T17:28:52.078485Z","shell.execute_reply.started":"2023-10-14T17:28:52.072522Z"},"papermill":{"duration":0.040256,"end_time":"2023-10-06T03:42:50.997687","exception":false,"start_time":"2023-10-06T03:42:50.957431","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def mixup_data(x, y, alpha=1.0, use_cuda=True):\n","    '''Returns mixed inputs, pairs of targets, and lambda'''\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","\n","    batch_size = x.size()[0]\n","    if use_cuda:\n","        index = torch.randperm(batch_size).cuda()\n","    else:\n","        index = torch.randperm(batch_size)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:52.081249Z","iopub.status.busy":"2023-10-14T17:28:52.080433Z","iopub.status.idle":"2023-10-14T17:28:52.118664Z","shell.execute_reply":"2023-10-14T17:28:52.117722Z","shell.execute_reply.started":"2023-10-14T17:28:52.081212Z"},"trusted":true},"outputs":[],"source":["def read_masks(seg):\n","    masks = np.empty((512, 512)) # Return an array of zeros with the same shape and type as a given array\n","    mask = (seg >= 128).astype(int)  # 將mask中像素值大於等於128的元素轉為整數1，否則數值是原來的整數0\n","    mask = 4 * mask[:, :, 0] + 2 * mask[:, :, 1] + mask[:, :, 2]\n","    masks[mask == 3] = 0  # (Cyan: 011) Urban land \n","    masks[mask == 6] = 1 # (Yellow: 110) Agriculture land \n","    masks[mask == 5] = 2  # (Purple: 101) Rangeland \n","    masks[mask == 2] = 3  # (Green: 010) Forest land \n","    masks[mask == 1] = 4  # (Blue: 001) Water \n","    masks[mask == 7] = 5  # (White: 111) Barren land \n","    masks[mask == 0] = 6  # (Black: 000) Unknown\n","    return masks"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:52.120725Z","iopub.status.busy":"2023-10-14T17:28:52.119872Z","iopub.status.idle":"2023-10-14T17:28:52.130815Z","shell.execute_reply":"2023-10-14T17:28:52.129693Z","shell.execute_reply.started":"2023-10-14T17:28:52.120695Z"},"papermill":{"duration":0.042125,"end_time":"2023-10-06T03:42:51.070037","exception":false,"start_time":"2023-10-06T03:42:51.027912","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# It is important to do data augmentation in training.\n","# However, not every augmentation is useful.\n","# Please think about what kind of augmentation is helpful for this recognition of this task.\n","train_tfm = transforms.Compose([\n","    # Resize the image into a fixed shape (height = width = 128)\n","    # transforms.RandomHorizontalFlip(), #影像 RandomHorizontalFlip 和 RandomVerticalFlip\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    # transforms.RandomHorizontalFlip(),\n","])\n","\n","train_target_tfm = transforms.Compose([\n","    # transforms.ToPILImage(),\n","    #影像 RandomHorizontalFlip 和 RandomVerticalFlip\n","    transforms.ToTensor(),\n","    transforms.RandomHorizontalFlip(),\n","])\n","\n","# We don't need augmentations in testing and validation.\n","# All we need here is to resize the PIL image and transform it into Tensor.\n","test_tfm = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","test_target_tfm = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.030029,"end_time":"2023-10-06T03:42:51.130972","exception":false,"start_time":"2023-10-06T03:42:51.100943","status":"completed"},"tags":[]},"source":["# Datasets"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:52.134616Z","iopub.status.busy":"2023-10-14T17:28:52.134073Z","iopub.status.idle":"2023-10-14T17:28:52.148023Z","shell.execute_reply":"2023-10-14T17:28:52.147152Z","shell.execute_reply.started":"2023-10-14T17:28:52.134592Z"},"trusted":true},"outputs":[],"source":["from PIL import Image\n","import torchvision\n","from torchvision import transforms as T\n","import random\n","\n","class RandomHorizontalFlip(object):\n","    def __init__(self, flip_prob=0.5):\n","        self.flip_prob = flip_prob\n"," \n","    def __call__(self, image, target=None):\n","        if random.random() < self.flip_prob:\n","            image = torchvision.transforms.functional.hflip(image)\n","            if target is not None:\n","                target = torchvision.transforms.functional.hflip(target)\n","        return image, target\n","random_horizontal_flip = RandomHorizontalFlip(flip_prob=0.5)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:52.149992Z","iopub.status.busy":"2023-10-14T17:28:52.149187Z","iopub.status.idle":"2023-10-14T17:28:52.160559Z","shell.execute_reply":"2023-10-14T17:28:52.159653Z","shell.execute_reply.started":"2023-10-14T17:28:52.149951Z"},"papermill":{"duration":0.039064,"end_time":"2023-10-06T03:42:51.199760","exception":false,"start_time":"2023-10-06T03:42:51.160696","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import imageio\n","\n","class hw1_3_Dataset(Dataset):\n","    import imageio\n","    def __init__(self,path,tfm=test_tfm, test=True):\n","        super(hw1_3_Dataset).__init__()\n","        self.path = path\n","        self.files = sorted([x.split(\".\")[0].split('_')[0] for x in os.listdir(path) if x.endswith(\".jpg\")])\n","        self.imgs = [os.path.join(path,(x)+\"_sat.jpg\") for x in self.files]\n","        self.masks = [os.path.join(path,(x)+\"_mask.png\") for x in self.files]\n","        \n","        self.transform = tfm\n","        self.test = test\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self,idx):\n","        img_name = self.imgs[idx] # fname = 31_327\n","        mask_name = self.masks[idx]\n","        im = Image.open(img_name)\n","        im = self.transform(im)\n","        if not self.test:\n","            try:\n","                mask = imageio.imread(mask_name)\n","                mask = read_masks(mask)\n","                mask = torch.from_numpy(mask).to(dtype=torch.long)\n","                im, mask = random_horizontal_flip(im, mask)\n","            except:\n","                mask = im\n","        else:\n","            mask = imageio.imread(mask_name)\n","            mask = read_masks(mask)\n","            mask = torch.from_numpy(mask).to(dtype=torch.long)\n","        \n","        return im,mask"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.0297,"end_time":"2023-10-06T03:42:57.677988","exception":false,"start_time":"2023-10-06T03:42:57.648288","status":"completed"},"tags":[]},"source":["# Dataloader(Mini-ImageNet)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:52.162312Z","iopub.status.busy":"2023-10-14T17:28:52.161658Z","iopub.status.idle":"2023-10-14T17:28:52.186055Z","shell.execute_reply":"2023-10-14T17:28:52.185279Z","shell.execute_reply.started":"2023-10-14T17:28:52.162280Z"},"papermill":{"duration":0.110322,"end_time":"2023-10-06T03:42:57.818675","exception":false,"start_time":"2023-10-06T03:42:57.708353","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Construct train and valid datasets.\n","# The argument \"loader\" tells how torchvision reads the data.\n","# The number of batch size.\n","batch_size = 16\n","train_set = hw1_3_Dataset(path=\"./hw1_data/p3_data/train\", tfm=train_tfm, test=False)\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n","\n","val_set = hw1_3_Dataset(path=\"./hw1_data/p3_data/validation\", tfm=test_tfm, test=True)\n","val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:52.187846Z","iopub.status.busy":"2023-10-14T17:28:52.187363Z","iopub.status.idle":"2023-10-14T17:28:56.998958Z","shell.execute_reply":"2023-10-14T17:28:56.997992Z","shell.execute_reply.started":"2023-10-14T17:28:52.187817Z"},"papermill":{"duration":1.171256,"end_time":"2023-10-06T07:57:27.336996","exception":false,"start_time":"2023-10-06T07:57:26.165740","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/deeplabv3_mobilenet_v3_large-fc3c493d.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_mobilenet_v3_large-fc3c493d.pth\n","100%|██████████| 42.3M/42.3M [00:00<00:00, 163MB/s]\n"]}],"source":["num_classes = 7\n","# \"cuda\" only when GPUs are available.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Initialize a model, and put it on the device specified.\n","vgg16_fcn32s = fcn32s(n_class=num_classes)\n","# model = vgg16_fcn32s\n","model = createDeepLabv3_mv3(outputchannels=num_classes)\n","# model = createDeepLabv3_r50(outputchannels=num_classes)\n","model.to(device)\n","\n","# The number of training epochs.\n","n_epochs = 80\n","\n","# weight decay\n","wd_num = 1e-5\n","\n","# MixUp alpha(α \\alphaα在0.2 ~ 2之間效果都差不多，表示mixup對α \\alphaα參數並不是很敏感。但如果α \\alphaα過小，等於沒有進行mixup的原始數據，如果α \\alphaα過大，等於所有輸入都是各取一半混合)\n","alpha = 0\n","\n","# If no improvement in 'patience' epochs, early stop.\n","patience = 15\n","\n","# For the classification task, we use cross-entropy as the measurement of performance.\n","criterion = nn.CrossEntropyLoss()\n","\n","# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=wd_num)\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=wd_num)\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.1, patience=10)\n","\n","ues_lrscheduling = True\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=10)\n","\n","\n","ckpt_name = \"exp8_createDeepLabv3_mv3_best.ckpt\"\n","first_ckpt_name = 'exp8_createDeepLabv3_mv3_first.ckpt '\n","middle_ckpt_name = 'exp8_createDeepLabv3_mv3_middle.ckpt '\n","final_ckpt_name = 'exp8_createDeepLabv3_mv3_final.ckpt '"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.030167,"end_time":"2023-10-06T03:42:51.260324","exception":false,"start_time":"2023-10-06T03:42:51.230157","status":"completed"},"tags":[]},"source":["# Create Model and Configurations"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-14T17:28:57.000799Z","iopub.status.busy":"2023-10-14T17:28:57.000478Z","iopub.status.idle":"2023-10-14T17:28:57.028124Z","shell.execute_reply":"2023-10-14T17:28:57.027274Z","shell.execute_reply.started":"2023-10-14T17:28:57.000766Z"},"trusted":true},"outputs":[],"source":["import numpy\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BinaryFocalLoss(nn.Module):\n","    \"\"\"\n","    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n","    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n","        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n","    :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n","    :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n","                    focus on hard misclassified example\n","    :param reduction: `none`|`mean`|`sum`\n","    :param **kwargs\n","        balance_index: (int) balance class index, should be specific when alpha is float\n","    \"\"\"\n","\n","    def __init__(self, alpha=0.25, gamma=2, ignore_index=None, reduction='mean', **kwargs):\n","        super(BinaryFocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.smooth = 1e-6  # set '1e-4' when train with FP16\n","        self.ignore_index = ignore_index\n","        self.reduction = reduction\n","\n","        assert self.reduction in ['none', 'mean', 'sum']\n","\n","        # if self.alpha is None:\n","        #     self.alpha = torch.ones(2)\n","        # elif isinstance(self.alpha, (list, np.ndarray)):\n","        #     self.alpha = np.asarray(self.alpha)\n","        #     self.alpha = np.reshape(self.alpha, (2))\n","        #     assert self.alpha.shape[0] == 2, \\\n","        #         'the `alpha` shape is not match the number of class'\n","        # elif isinstance(self.alpha, (float, int)):\n","        #     self.alpha = np.asarray([self.alpha, 1.0 - self.alpha], dtype=np.float).view(2)\n","\n","        # else:\n","        #     raise TypeError('{} not supported'.format(type(self.alpha)))\n","\n","    def forward(self, output, target):\n","        prob = torch.sigmoid(output)\n","        prob = torch.clamp(prob, self.smooth, 1.0 - self.smooth)\n","\n","        valid_mask = None\n","        if self.ignore_index is not None:\n","            valid_mask = (target != self.ignore_index).float()\n","\n","        pos_mask = (target == 1).float()\n","        neg_mask = (target == 0).float()\n","        if valid_mask is not None:\n","            pos_mask = pos_mask * valid_mask\n","            neg_mask = neg_mask * valid_mask\n","\n","        pos_weight = (pos_mask * torch.pow(1 - prob, self.gamma)).detach()\n","        pos_loss = -pos_weight * torch.log(prob)  # / (torch.sum(pos_weight) + 1e-4)\n","\n","        neg_weight = (neg_mask * torch.pow(prob, self.gamma)).detach()\n","        neg_loss = -self.alpha * neg_weight * F.logsigmoid(-output)  # / (torch.sum(neg_weight) + 1e-4)\n","        loss = pos_loss + neg_loss\n","        loss = loss.mean()\n","        return loss\n","\n","\n","class FocalLoss_Ori(nn.Module):\n","    \"\"\"\n","    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n","    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n","    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n","    Args:\n","        num_class: number of classes\n","        alpha: class balance factor\n","        gamma:\n","        ignore_index:\n","        reduction:\n","    \"\"\"\n","\n","    def __init__(self, num_class, alpha=None, gamma=2, ignore_index=None, reduction='mean'):\n","        super(FocalLoss_Ori, self).__init__()\n","        self.num_class = num_class\n","        self.gamma = gamma\n","        self.reduction = reduction\n","        self.smooth = 1e-4\n","        self.ignore_index = ignore_index\n","        self.alpha = alpha\n","        if alpha is None:\n","            self.alpha = torch.ones(num_class, )\n","        elif isinstance(alpha, (int, float)):\n","            self.alpha = torch.as_tensor([alpha] * num_class)\n","        elif isinstance(alpha, (list, np.ndarray)):\n","            self.alpha = torch.as_tensor(alpha)\n","        if self.alpha.shape[0] != num_class:\n","            raise RuntimeError('the length not equal to number of class')\n","\n","        # if isinstance(self.alpha, (list, tuple, np.ndarray)):\n","        #     assert len(self.alpha) == self.num_class\n","        #     self.alpha = torch.Tensor(list(self.alpha))\n","        # elif isinstance(self.alpha, (float, int)):\n","        #     assert 0 < self.alpha < 1.0, 'alpha should be in `(0,1)`)'\n","        #     assert balance_index > -1\n","        #     alpha = torch.ones((self.num_class))\n","        #     alpha *= 1 - self.alpha\n","        #     alpha[balance_index] = self.alpha\n","        #     self.alpha = alpha\n","        # elif isinstance(self.alpha, torch.Tensor):\n","        #     self.alpha = self.alpha\n","        # else:\n","        #     raise TypeError('Not support alpha type, expect `int|float|list|tuple|torch.Tensor`')\n","\n","    def forward(self, logit, target):\n","        # assert isinstance(self.alpha,torch.Tensor)\\\n","        N, C = logit.shape[:2]\n","        alpha = self.alpha.to(logit.device)\n","        prob = F.softmax(logit, dim=1)\n","        if prob.dim() > 2:\n","            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n","            prob = prob.view(N, C, -1)\n","            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n","            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n","        ori_shp = target.shape\n","        target = target.view(-1, 1)  # [N,d1,d2,...]->[N*d1*d2*...,1]\n","        valid_mask = None\n","        if self.ignore_index is not None:\n","            valid_mask = target != self.ignore_index\n","            target = target * valid_mask\n","\n","        # ----------memory saving way--------\n","        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n","        logpt = torch.log(prob)\n","        # alpha_class = alpha.gather(0, target.view(-1))\n","        alpha_class = alpha[target.squeeze().long()]\n","        class_weight = -alpha_class * torch.pow(torch.sub(1.0, prob), self.gamma)\n","        loss = class_weight * logpt\n","        if valid_mask is not None:\n","            loss = loss * valid_mask.squeeze()\n","\n","        if self.reduction == 'mean':\n","            loss = loss.mean()\n","            if valid_mask is not None:\n","                loss = loss.sum() / valid_mask.sum()\n","        elif self.reduction == 'none':\n","            loss = loss.view(ori_shp)\n","        return loss\n","    \n","FL_criterion = FocalLoss_Ori(num_class=7, alpha=0.25)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":5769.68168,"end_time":"2023-10-06T09:33:40.812924","exception":false,"start_time":"2023-10-06T07:57:31.131244","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["## Initialize trackers, these are not parameters and should not be changed\n","stale = 0\n","best_mIoU = 0\n","\n","for epoch in range(n_epochs):\n","\n","    # ---------- Training ----------\n","    # Make sure the model is in train mode before training.\n","    model.train()\n","\n","    # These are used to record information in training.\n","    train_loss = []\n","    tp_fp = np.zeros(6)\n","    tp_fn = np.zeros(6)\n","    tp = np.zeros(6)\n","    mean_iou = 0\n","    for batch in tqdm(train_loader):\n","\n","        # A batch consists of image data and corresponding labels.\n","        imgs, labels = batch\n","        #imgs = imgs.half()\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        # imgs, targets_a, targets_b, lam = mixup_data(imgs, labels, alpha)\n","        # Forward the data. (Make sure data and model are on the same device.)\n","        logits = model(imgs)['out']\n","        # Calculate the cross-entropy loss.\n","        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n","        # loss = mixup_criterion(criterion, logits, targets_a, targets_b, lam)\n","        loss = criterion(logits, labels)\n","\n","        # Gradients stored in the parameters in the previous step should be cleared out first.\n","        optimizer.zero_grad()\n","\n","        # Compute the gradients for parameters.\n","        loss.backward()\n","\n","        # Clip the gradient norms for stable training.\n","        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","\n","        # Update the parameters with computed gradients.\n","        optimizer.step()\n","    \n","        # Compute the accuracy for current batch.\n","        # acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n","        label_pred = logits.argmax(dim=1).detach().cpu().numpy()\n","        \n","        label_true = labels.detach().cpu().numpy()\n","        # Record the loss and accuracy.\n","        train_loss.append(loss.item())    \n","\n","        for i in range(6):\n","            tp_fp_num = np.sum(label_pred == i)\n","            tp_fn_num = np.sum(label_true == i)\n","            tp_num = np.sum((label_pred == i) * (label_true == i))\n","            tp_fp[i] += tp_fp_num\n","            tp_fn[i] += tp_fn_num\n","            tp[i] += tp_num\n","    print(tp_fp)      \n","    for i in range(6):\n","        iou = tp[i] / (tp_fp[i] + tp_fn[i] - tp[i])\n","        mean_iou += iou / 6\n","        print('class #%d : %1.5f'%(i, iou))\n","    print('\\ntrain_mean_iou: %f\\n' % mean_iou)\n","    \n","    train_loss = sum(train_loss) / len(train_loss)\n","\n","    # Print the information.\n","    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, mIoU = {mean_iou:.5f}\")\n","    \n","    if ues_lrscheduling:\n","        scheduler.step(train_loss)\n","        \n","    # ---------- Validation ----------\n","    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n","    model.eval()\n","\n","    # These are used to record information in validation.\n","    valid_loss = []\n","    tp_fp = np.zeros(6)\n","    tp_fn = np.zeros(6)\n","    tp = np.zeros(6)\n","    mean_iou = 0\n","\n","    # Iterate the validation set by batches.\n","    for batch in tqdm(val_loader):\n","\n","        # A batch consists of image data and corresponding labels.\n","        imgs, labels = batch\n","        #imgs = imgs.half()\n","\n","        # We don't need gradient in validation.\n","        # Using torch.no_grad() accelerates the forward process.\n","        with torch.no_grad():\n","            logits = model(imgs.to(device))['out']\n","        \n","        # We can still compute the loss (but not the gradient).\n","        loss = criterion(logits, labels.to(device))\n","\n","        # Compute the accuracy for current batch.\n","        label_pred = logits.argmax(dim=1).detach().cpu().numpy()\n","        label_true = labels.detach().cpu().numpy()\n","\n","        # Record the loss and accuracy.\n","        valid_loss.append(loss.item())\n","        for i in range(6):\n","            tp_fp_num = np.sum(label_pred == i)\n","            tp_fn_num = np.sum(label_true == i)\n","            tp_num = np.sum((label_pred == i) * (label_true == i))\n","            tp_fp[i] += tp_fp_num\n","            tp_fn[i] += tp_fn_num\n","            tp[i] += tp_num\n","        #break\n","\n","    # The average loss and accuracy for entire validation set is the average of the recorded values.\n","    valid_loss = sum(valid_loss) / len(valid_loss)\n","    \n","    print(tp_fp)      \n","    for i in range(6):\n","        iou = tp[i] / (tp_fp[i] + tp_fn[i] - tp[i])\n","        mean_iou += iou / 6\n","        print('class #%d : %1.5f'%(i, iou))\n","    print('\\nvalidation_mean_iou: %f\\n' % mean_iou)\n","\n","    # Print the information.\n","    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, mIoU = {mean_iou:.5f}\")\n","        \n","    # update logs\n","    if mean_iou > best_mIoU:\n","        with open(\"./sample_best_log.txt\",\"a\"):\n","            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, mIoU = {mean_iou:.5f} -> best\")\n","    else:\n","        with open(\"./sample_best_log.txt\",\"a\"):\n","            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, mIoU = {mean_iou:.5f}\")\n","\n","    # save models\n","    if mean_iou > best_mIoU:\n","        best_epoch = epoch+1\n","        print(f\"Best model found at epoch {best_epoch}, saving model\")\n","        torch.save(model.state_dict(), ckpt_name) # only save best to prevent output memory exceed error\n","        best_mIoU = mean_iou\n","        stale = 0\n","    else:\n","        stale += 1\n","        if stale > patience:\n","            print(f\"No improvment {patience} consecutive epochs, early stopping at no.{epoch+1} epoch, best epoch at {best_epoch}.\")\n","            break\n","            \n","    if (epoch+1) == 1:\n","        print(f\"First model found at epoch {epoch+1}, saving model\")\n","        torch.save(model.state_dict(), first_ckpt_name)\n","    if (epoch+1) == (2):\n","        print(f\"Middle model found at epoch {epoch+1}, saving model\")\n","        torch.save(model.state_dict(), middle_ckpt_name)\n","    if (epoch+1) == n_epochs:\n","        print(f\"Final model found at epoch {epoch+1}, saving model\")\n","        torch.save(model.state_dict(), final_ckpt_name)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.504807,"end_time":"2023-10-06T07:57:30.530485","exception":false,"start_time":"2023-10-06T07:57:30.025678","status":"completed"},"tags":[]},"source":["# Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_classes = 7\n","# \"cuda\" only when GPUs are available.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","model = createDeepLabv3_mv3(outputchannels=num_classes)\n","model.to(device)\n","model.eval()\n","model.load_state_dict(torch.load('/kaggle/input/exp2-createdeeplabv3-mv3-best-ckpt/exp2_createDeepLabv3_mv3_best.ckpt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from PIL import Image\n","from torchvision import transforms\n","filename = ['./hw1_data/p3_data/validation/0013_sat.jpg','./hw1_data/p3_data/validation/0062_sat.jpg','./hw1_data/p3_data/validation/0104_sat.jpg']\n","filename = filename[1]\n","input_image = Image.open(filename)\n","input_image = input_image.convert(\"RGB\")\n","input_tensor = test_tfm(input_image)\n","input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n","\n","input_batch = input_batch.to(device)\n","\n","with torch.no_grad():\n","    output = model(input_batch)['out'][0]\n","output_predictions = output.argmax(0).detach().cpu().numpy()\n","viz_masks = np.repeat(output_predictions[:, :, np.newaxis], 3, axis=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import argparse\n","import scipy.ndimage\n","import imageio.v2 as imageio\n","\n","import numpy as np \n","\n","from matplotlib import colors as mcolors\n","\n","voc_cls = {'urban': 0, \n","           'rangeland': 2,\n","           'forest': 3,  \n","           'unknown': 6,  \n","           'barreb land': 5,  \n","           'Agriculture land': 1,  \n","           'water': 4,\n","           } \n","cls_color = {\n","    0:  [0, 255, 255],\n","    1:  [255, 255, 0],\n","    2:  [255, 0, 255],\n","    3:  [0, 255, 0],\n","    4:  [0, 0, 255],\n","    5:  [255, 255, 255],\n","    6: [0, 0, 0],\n","}\n","def mask_edge_detection(mask, edge_width):\n","    h = mask.shape[0]\n","    w = mask.shape[1]\n","\n","    edge_mask = np.zeros((h,w))\n","    \n","    for i in range(h):\n","        for j in range(1,w):\n","            j_prev = j - 1 \n","            # horizontal #\n","            if not mask[i][j] == mask[i][j_prev]: # horizontal\n","                if mask[i][j]==1: # 0 -> 1\n","                    edge_mask[i][j] = 1\n","                    for add in range(1,edge_width):\n","                        if j + add < w and mask[i][j+add] == 1:\n","                            edge_mask[i][j+add] = 1\n","                        \n","                else : # 1 -> 0\n","                    edge_mask[i][j_prev] = 1\n","                    for minus in range(1,edge_width):\n","                        if j_prev - minus >= 0 and mask[i][j_prev - minus] == 1: \n","                            edge_mask[i][j_prev - minus] = 1\n","            # vertical #\n","            if not i == 0 :\n","                i_prev = i - 1\n","                if not mask[i][j] == mask[i_prev][j]: \n","                    if mask[i][j]==1: # 0 -> 1\n","                        edge_mask[i][j] = 1 \n","                        for add in range(1,edge_width):\n","                            if i + add < h and mask[i+add][j] == 1:\n","                                edge_mask[i+add][j] = 1 \n","                    else : # 1 -> 0\n","                        edge_mask[i_prev][j] = 1\n","                        for minus in range(1,edge_width):\n","                            if i_prev - minus >= 0 and mask[i_prev-minus][j] == 1:\n","                                edge_mask[i_prev-minus][j] == 1\n","    return edge_mask\n","\n","def viz_data(im, seg, color, inner_alpha = 0.3, edge_alpha = 1, edge_width = 5):\n","     \n","    edge = mask_edge_detection(seg, edge_width)\n","\n","    color_mask = np.zeros((edge.shape[0]*edge.shape[1], 3))\n","    l_loc = np.where(seg.flatten() == 1)[0]\n","    color_mask[l_loc, : ] = color\n","    color_mask = np.reshape(color_mask, im.shape)\n","    mask = np.concatenate((seg[:,:,np.newaxis],seg[:,:,np.newaxis],seg[:,:,np.newaxis]), axis = -1)\n","    \n","    color_edge = np.zeros((edge.shape[0]*edge.shape[1], 3))\n","    l_col = np.where(edge.flatten() == 1)[0]\n","    color_edge[l_col,:] = color\n","    color_edge = np.reshape(color_edge, im.shape)\n","    edge = np.concatenate((edge[:,:,np.newaxis],edge[:,:,np.newaxis],edge[:,:,np.newaxis]), axis = -1)\n","\n","\n","    im_new = im*(1-mask) + im*mask*(1-inner_alpha) + color_mask * inner_alpha\n","    im_new =  im_new*(1-edge) + im_new*edge*(1-edge_alpha) + color_edge*edge_alpha\n","\n","    return im_new \n","\n","def arg_parse():\n","    parser = argparse.ArgumentParser(description='Tools to visualize semantic segmentation map.')\n","\n","    # Datasets parameters\n","    parser.add_argument('--sat_path', type=str, default='', \n","                    help=\"path to RGB satellite image\")\n","    parser.add_argument('--mask_path', type=str, default='', \n","                    help=\"path to mask\")\n","\n","    args = parser.parse_args()\n","\n","    return args\n","\n","def read_masks(seg):\n","    masks = np.zeros_like(seg) # Return an array of zeros with the same shape and type as a given array\n","    mask = (seg >= 128).astype(int)  # 將mask中像素值大於等於128的元素轉為整數1，否則數值是原來的整數0\n","    mask = 4 * mask[:, :, 0] + 2 * mask[:, :, 1] + mask[:, :, 2]\n","    masks[mask == 3] = 0  # (Cyan: 011) Urban land \n","    masks[mask == 6] = 1  # (Yellow: 110) Agriculture land \n","    masks[mask == 5] = 2  # (Purple: 101) Rangeland \n","    masks[mask == 2] = 3  # (Green: 010) Forest land \n","    masks[mask == 1] = 4  # (Blue: 001) Water \n","    masks[mask == 7] = 5  # (White: 111) Barren land \n","    masks[mask == 0] = 6  # (Black: 000) Unknown\n","    return masks\n","\n","\n","\n","if __name__ == '__main__':\n","    cmap = cls_color\n","    img = imageio.imread(filename)\n","    masks = viz_masks\n","    cs = np.unique(masks)\n","\n","    for c in cs:\n","        mask = np.zeros((img.shape[0], img.shape[1]))\n","        ind = np.where(masks == c)\n","        mask[ind[0], ind[1]] = 1\n","        img = viz_data(img, mask, color=cmap[c])\n","        imageio.imsave('./exp.png', np.uint8(img))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
